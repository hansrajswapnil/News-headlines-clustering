{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel(r'news_headlines.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.DataFrame(df, columns=['SENTENCES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Moeller's student-run newspaper, The Crusader,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In 2008, The Crusader won First Place, the sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The Squire is a student literary journal that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Paul Keels - play-by-play announcer for Ohio S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Joe Uecker - Ohio State Senator (R-66) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46112</td>\n",
       "      <td>Vancouver's characteristic approach to urban p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46113</td>\n",
       "      <td>Vancouver is also considered to have the worst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46114</td>\n",
       "      <td>The Vancouver Art Gallery is housed downtown i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46115</td>\n",
       "      <td>A prominent addition to the city's landscape i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46116</td>\n",
       "      <td>A collection of Edwardian buildings in the cit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46117 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               SENTENCES\n",
       "0      Moeller's student-run newspaper, The Crusader,...\n",
       "1      In 2008, The Crusader won First Place, the sec...\n",
       "2      The Squire is a student literary journal that ...\n",
       "3      Paul Keels - play-by-play announcer for Ohio S...\n",
       "4               Joe Uecker - Ohio State Senator (R-66) .\n",
       "...                                                  ...\n",
       "46112  Vancouver's characteristic approach to urban p...\n",
       "46113  Vancouver is also considered to have the worst...\n",
       "46114  The Vancouver Art Gallery is housed downtown i...\n",
       "46115  A prominent addition to the city's landscape i...\n",
       "46116  A collection of Edwardian buildings in the cit...\n",
       "\n",
       "[46117 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\",\", \"(\", \")\" ,\".\" ,\"-\" ,\"i\" , \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = settings['n']  #dim of word embeddings\n",
    "        self.lr = settings['learning_rate']\n",
    "        self.epochs = settings['epochs']    \n",
    "        self.window = settings['window_size']   #context window +- center word\n",
    "\n",
    "    \n",
    "    def generate_training_data(self, settings, corpus):\n",
    "        word_count = defaultdict(int)  #finds unq word counts using dictionary\n",
    "        \n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_count[word] += 1\n",
    "\n",
    "        self.v_count = len(word_count.keys())   #how many unq words in the dict\n",
    "        print(self.v_count)\n",
    "        self.word_list = list(word_count.keys())    #generatings look-up dict(vocab)\n",
    "        \n",
    "        #generate word:index\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.word_list))\n",
    "\n",
    "        #generate index:word\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.word_list))\n",
    "\n",
    "\n",
    "        training_data = []\n",
    "\n",
    "        #cycling through each sentence in corpus\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "            \n",
    "            #cyle through each word in sentence\n",
    "            for i, word in enumerate(sentence):\n",
    "\n",
    "                #converting target word into one-hot enc\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "\n",
    "                w_context= []\n",
    "\n",
    "                #cycle through context window\n",
    "                for j in range(i-self.window, i+ self.window+1):\n",
    "                    #criteria for context word\n",
    "                    #1. target word != context word\n",
    "                    #2. index must be >= 0 (j >= 0)\n",
    "                    #3. index must be <= len(sentence)\n",
    "\n",
    "                    if j != i and j<=sent_len-1 and j>=0:\n",
    "                        \n",
    "                        #append the one-hot representation of word to w_context\n",
    "                        w_context.append(self.word2onehot(sentence[j]))\n",
    "\n",
    "                \n",
    "                training_data.append([w_target, w_context])\n",
    "\n",
    "        return np.array(training_data)\n",
    "\n",
    "    def word2onehot(self, word):\n",
    "\n",
    "        #initialize a blank vector __ word_vec\n",
    "        word_vec = [0 for i in range(0, self.v_count)]\n",
    "\n",
    "        #get the ID of the word from word_index\n",
    "        word_index = self.word_index[word]\n",
    "        \n",
    "        #change value to 1 acc to ID of the word\n",
    "        word_vec[word_index] = 1\n",
    "\n",
    "        return word_vec\n",
    "\n",
    "    def train(self, training_data):\n",
    "        \n",
    "        #initialize weight matrix\n",
    "        self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))      #9x10\n",
    "        self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))      #10x9\n",
    "\n",
    "\n",
    "        #cycle through each epoch\n",
    "        for i in range(self.epochs):\n",
    "            self.loss = 0       #initialize loss to 0\n",
    "\n",
    "            #cycle through each training example\n",
    "            for w_t, w_c in training_data:      #w_t is the target vector & w_c is the context vector\n",
    "\n",
    "                #forward pass\n",
    "                y_pred, h, u = self.forward_pass(w_t)\n",
    "                \n",
    "                #calculate error\n",
    "                #for a target word, cal diff b/w y_pred & each of the context words\n",
    "                #sum up the diffreneces for each target word\n",
    "                EI = np.sum([np.subtract(y_pred,word) for word in w_c],axis=0)\n",
    "\n",
    "                #backpropagation\n",
    "                #we use SGD to backpropagate errors - cal loss on the output layer\n",
    "                self.backprop(EI, h, w_t)\n",
    "                \n",
    "                #calculate loss\n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "\n",
    "            print('\\nEpoch:', i, 'Loss: ', self.loss)\n",
    "\n",
    "    def forward_pass(self, x) :\n",
    "        #hidden layer activation\n",
    "        h = np.dot(self.w1.T, x )\n",
    "        \n",
    "        #output layer before softmax \n",
    "        u = np.dot(self.w2.T, h)\n",
    "\n",
    "        #run output layer through softmax\n",
    "        y_c = self.softmax(u)\n",
    "\n",
    "        return y_c, h, u\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x/e_x.sum(axis=0)\n",
    "\n",
    "    def backprop(self, e, h, x):\n",
    "        dl_dw2 = np.outer(h, e)     #d1_dw2 : (9x1) X (10x1)\n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e))      #x --- 9x1 ; self.w2 --- 9x10, e.T --- 1x9, e --- 9x1, self.w2.T --- 10x9\n",
    "        #d1_dw1 : 9x10\n",
    "        #update weights\n",
    "        self.w1 = self.w1 - (self.lr * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.lr * dl_dw2)\n",
    "\n",
    "    #get vector from the word\n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_index[word]\n",
    "        v_w = self.w1[w_index]\n",
    "        return v_w\n",
    "\n",
    "    #input vector retunr nearest word(s)\n",
    "    def vec_sim(self, word, top_n):\n",
    "        v_w1 = self.word_vec(word)\n",
    "        word_sim = {}\n",
    "\n",
    "        for i in range(self.v_count):\n",
    "            #find the similarity score for each word in vocab\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_sum = np.dot(v_w1, v_w2)\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_sum / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "\n",
    "        word_sorted = sorted(word_sim.items(), key=lambda kv : kv[1], reverse=True)\n",
    "\n",
    "        for word, sim in word_sorted[:top_n]:\n",
    "            print('\\n')\n",
    "            print(word, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation mode of Image forming of the AFM are generally classified into two groups from the viewpoint whether it uses z-Feedback loop (not shown) to maintain the tip-sample distance to keep signal intensity exported by the detector.\n",
      "233\n"
     ]
    }
   ],
   "source": [
    "for ind in df.index:\n",
    "    if ind == 0:\n",
    "        text = df['SENTENCES'][100]\n",
    "print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['operation', 'mode', 'of', 'image', 'forming', 'of', 'the', 'afm', 'are', 'generally', 'classified', 'into', 'two', 'groups', 'from', 'the', 'viewpoint', 'whether', 'it', 'uses', 'z-feedback', 'loop', '(not', 'shown)', 'to', 'maintain', 'the', 'tip-sample', 'distance', 'to', 'keep', 'signal', 'intensity', 'exported', 'by', 'the', 'detector.']\n"
     ]
    }
   ],
   "source": [
    "corpus = [[word.lower() for word in text.split()]]\n",
    "print(len(corpus[0]))\n",
    "print(corpus[0])\n",
    "i =0\n",
    "for word in stopwords:\n",
    "    while i < len(corpus[0]):\n",
    "        if corpus[0][i] == word:\n",
    "            corpus[0].pop(i)\n",
    "print()\n",
    "print(corpus)\n",
    "print(len(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus\n",
    "corpus[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('window_size', 2), ('n', 31), ('epochs', 100), ('learning_rate', 0.07)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = {\n",
    "    'window_size':2,\n",
    "    'n': len(corpus[0]),\n",
    "    'epochs':100,\n",
    "    'learning_rate':0.07\n",
    "}\n",
    "settings.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "\n",
      "Epoch: 0 Loss:  599.2618029950265\n",
      "\n",
      "Epoch: 1 Loss:  381.5696754215314\n",
      "\n",
      "Epoch: 2 Loss:  295.17108915821746\n",
      "\n",
      "Epoch: 3 Loss:  247.68348752114815\n",
      "\n",
      "Epoch: 4 Loss:  218.72874053445074\n",
      "\n",
      "Epoch: 5 Loss:  201.60588676269913\n",
      "\n",
      "Epoch: 6 Loss:  191.82900880184744\n",
      "\n",
      "Epoch: 7 Loss:  185.9962282018811\n",
      "\n",
      "Epoch: 8 Loss:  182.24187469921793\n",
      "\n",
      "Epoch: 9 Loss:  179.68066935412315\n",
      "\n",
      "Epoch: 10 Loss:  177.84278646092284\n",
      "\n",
      "Epoch: 11 Loss:  176.45969387182802\n",
      "\n",
      "Epoch: 12 Loss:  175.390921272997\n",
      "\n",
      "Epoch: 13 Loss:  174.53463608645868\n",
      "\n",
      "Epoch: 14 Loss:  173.85099266257313\n",
      "\n",
      "Epoch: 15 Loss:  173.28433556141636\n",
      "\n",
      "Epoch: 16 Loss:  172.83958896198078\n",
      "\n",
      "Epoch: 17 Loss:  172.45904917696345\n",
      "\n",
      "Epoch: 18 Loss:  172.1768525472945\n",
      "\n",
      "Epoch: 19 Loss:  171.8970158775967\n",
      "\n",
      "Epoch: 20 Loss:  171.69713413850857\n",
      "\n",
      "Epoch: 21 Loss:  171.42764478733775\n",
      "\n",
      "Epoch: 22 Loss:  171.25090528321223\n",
      "\n",
      "Epoch: 23 Loss:  170.95872312173267\n",
      "\n",
      "Epoch: 24 Loss:  170.79684331457196\n",
      "\n",
      "Epoch: 25 Loss:  170.49747717668123\n",
      "\n",
      "Epoch: 26 Loss:  170.35818411312223\n",
      "\n",
      "Epoch: 27 Loss:  170.06728772366785\n",
      "\n",
      "Epoch: 28 Loss:  169.95693067566023\n",
      "\n",
      "Epoch: 29 Loss:  169.68045715646383\n",
      "\n",
      "Epoch: 30 Loss:  169.60789962986857\n",
      "\n",
      "Epoch: 31 Loss:  169.34899960062046\n",
      "\n",
      "Epoch: 32 Loss:  169.33361366643285\n",
      "\n",
      "Epoch: 33 Loss:  169.10367672034133\n",
      "\n",
      "Epoch: 34 Loss:  169.18439832512794\n",
      "\n",
      "Epoch: 35 Loss:  169.01704125122478\n",
      "\n",
      "Epoch: 36 Loss:  169.24578211384096\n",
      "\n",
      "Epoch: 37 Loss:  169.1846864038173\n",
      "\n",
      "Epoch: 38 Loss:  169.5494511481334\n",
      "\n",
      "Epoch: 39 Loss:  169.55514173162587\n",
      "\n",
      "Epoch: 40 Loss:  169.9211454063675\n",
      "\n",
      "Epoch: 41 Loss:  169.8528684382774\n",
      "\n",
      "Epoch: 42 Loss:  170.11770200295797\n",
      "\n",
      "Epoch: 43 Loss:  169.90250135094925\n",
      "\n",
      "Epoch: 44 Loss:  170.05540701557982\n",
      "\n",
      "Epoch: 45 Loss:  169.7350810732555\n",
      "\n",
      "Epoch: 46 Loss:  169.82498344777895\n",
      "\n",
      "Epoch: 47 Loss:  169.46182599849402\n",
      "\n",
      "Epoch: 48 Loss:  169.53431986611832\n",
      "\n",
      "Epoch: 49 Loss:  169.16398055975804\n",
      "\n",
      "Epoch: 50 Loss:  169.2329217951262\n",
      "\n",
      "Epoch: 51 Loss:  168.8711522765996\n",
      "\n",
      "Epoch: 52 Loss:  168.93607121106558\n",
      "\n",
      "Epoch: 53 Loss:  168.5906379335794\n",
      "\n",
      "Epoch: 54 Loss:  168.64825031628627\n",
      "\n",
      "Epoch: 55 Loss:  168.3234809364009\n",
      "\n",
      "Epoch: 56 Loss:  168.37113909680258\n",
      "\n",
      "Epoch: 57 Loss:  168.06907342005255\n",
      "\n",
      "Epoch: 58 Loss:  168.1052686416253\n",
      "\n",
      "Epoch: 59 Loss:  167.82687889580666\n",
      "\n",
      "Epoch: 60 Loss:  167.85141283708592\n",
      "\n",
      "Epoch: 61 Loss:  167.5992305109583\n",
      "\n",
      "Epoch: 62 Loss:  167.6128052922804\n",
      "\n",
      "Epoch: 63 Loss:  167.39220821170986\n",
      "\n",
      "Epoch: 64 Loss:  167.3955731840755\n",
      "\n",
      "Epoch: 65 Loss:  167.2136618751395\n",
      "\n",
      "Epoch: 66 Loss:  167.2070314514813\n",
      "\n",
      "Epoch: 67 Loss:  167.07294820200082\n",
      "\n",
      "Epoch: 68 Loss:  167.05781851249824\n",
      "\n",
      "Epoch: 69 Loss:  166.98845952427243\n",
      "\n",
      "Epoch: 70 Loss:  166.97341540695308\n",
      "\n",
      "Epoch: 71 Loss:  167.0013667824942\n",
      "\n",
      "Epoch: 72 Loss:  167.00438888176976\n",
      "\n",
      "Epoch: 73 Loss:  167.16047295250712\n",
      "\n",
      "Epoch: 74 Loss:  167.17986412449204\n",
      "\n",
      "Epoch: 75 Loss:  167.41789088181423\n",
      "\n",
      "Epoch: 76 Loss:  167.39458954260925\n",
      "\n",
      "Epoch: 77 Loss:  167.5939558087432\n",
      "\n",
      "Epoch: 78 Loss:  167.4871398293112\n",
      "\n",
      "Epoch: 79 Loss:  167.60930221773276\n",
      "\n",
      "Epoch: 80 Loss:  167.44032054649145\n",
      "\n",
      "Epoch: 81 Loss:  167.52267338823924\n",
      "\n",
      "Epoch: 82 Loss:  167.33330090133816\n",
      "\n",
      "Epoch: 83 Loss:  167.39924660135097\n",
      "\n",
      "Epoch: 84 Loss:  167.22588726906227\n",
      "\n",
      "Epoch: 85 Loss:  167.27005961153057\n",
      "\n",
      "Epoch: 86 Loss:  167.12137411438212\n",
      "\n",
      "Epoch: 87 Loss:  167.12667552586026\n",
      "\n",
      "Epoch: 88 Loss:  167.00247794745286\n",
      "\n",
      "Epoch: 89 Loss:  166.95646915074647\n",
      "\n",
      "Epoch: 90 Loss:  166.86575444960607\n",
      "\n",
      "Epoch: 91 Loss:  166.7702677758695\n",
      "\n",
      "Epoch: 92 Loss:  166.73333395089873\n",
      "\n",
      "Epoch: 93 Loss:  166.59977059835234\n",
      "\n",
      "Epoch: 94 Loss:  166.64153255204226\n",
      "\n",
      "Epoch: 95 Loss:  166.48826898502043\n",
      "\n",
      "Epoch: 96 Loss:  166.6236276921752\n",
      "\n",
      "Epoch: 97 Loss:  166.4743874968955\n",
      "\n",
      "Epoch: 98 Loss:  166.69551792767123\n",
      "\n",
      "Epoch: 99 Loss:  166.56277006702598\n"
     ]
    }
   ],
   "source": [
    "w2v = word2vec()\n",
    "\n",
    "training_data = w2v.generate_training_data(settings, corpus)\n",
    "\n",
    "w2v.train(training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
